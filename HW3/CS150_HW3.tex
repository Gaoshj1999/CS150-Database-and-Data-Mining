%!TEX program = xelatex
\documentclass[10pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{titlesec}
\usepackage{xcolor}
%\usepackage[shortlabels]{enumitem}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{tikz}
\usepackage{listings}
\usetikzlibrary{arrows}
\usepackage{subfigure}
\usepackage{graphicx,booktabs,multirow}
\usepackage[a4paper]{geometry}
\usepackage{upquote}
\usepackage{float}
\usepackage{pdfpages}

\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\lstset{language=Matlab}
\lstset{breaklines}

\input defs.tex

\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

\titleformat*{\section}{\centering\LARGE\scshape}
\renewcommand{\thesection}{\Roman{section}}
\lstset{language=Matlab,tabsize=4,frame=shadowbox,basicstyle=\footnotesize,
keywordstyle=\color{blue!90}\bfseries,breaklines=true,commentstyle=\color[RGB]{50,50,50},stringstyle=\ttfamily,numbers=left,numberstyle=\tiny,
  numberstyle={\color[RGB]{192,92,92}\tiny},backgroundcolor=\color[RGB]{245,245,244},inputpath=code}

\begin{document}

\date{\today}
\title{Database and Data Mining, Fall 2020 \\
	Homework 3\\
	\small (Due Friday, Dec. 25 at 11:59pm (CST))}
\maketitle

\emph{Note that: solutions with the correct answer but without adequate explanation will not earn marks.}

\begin{enumerate}[1.]

	\item Use the $k$-means algorithm and Euclidean distance to cluster the following 8 data points:
	      \begin{align*}
		      x_1 & = (2,10), \ x_2 = (2,5), \ x_3 = (8,4), \ x_4 = (5,8), \\
		      x_5 & = (7,5),  \ x_6 = (6,4), \ x_7 = (1,2), \ x_8 = (4,9).
	      \end{align*}
	      Suppose the number of clusters is 3, and the Lloyd's algorithm is applied with the initial cluster centers $x_1$, $x_4$ and $x_7$.
	      At the end of the first iteration show:
	      \begin{itemize}
		      \item[(a)] The new clusters, i.e., the example assignment.  ~\defpoints{4}
		      \item[(b)] The centers of the new clusters. ~\defpoints{4}
		      \item[(c)] Draw a 10 by 10 space with all the 8 points, and show the clusters after the first iteration and the new centroids. ~\defpoints{4}
		      \item[(d)] How many more iterations are needed to converge? Draw the result for each iteration. ~\defpoints{8}
	      \end{itemize}


	\item  Given a set of i.i.d. observation pairs $(x_{1},y_{1})\cdots(x_{n},y_{n})$, where $x_i, y_i \in \mathbb{R}$, $i=1,2,...,n$.
	      \begin{itemize}
		      \item[(a)] By assuming the linear model is a reasonable approximation, we consider fitting the model via least squares approaches,
		            in which we choose coefficients $\theta$ and $\theta_0$ to minimize the Residual Sum of Squares (RSS),
		            \begin{equation}\label{eq:1}
			            \hat{\theta},~ \hat{\theta}_0 = \argmin_{\theta,~ \theta_0}~ \sum_{i=1}^{n}(y_{i} -\theta x_{i}-\theta_0)^{2}.
		            \end{equation}
		            Estimate the model parameters $\theta$ and $\theta_0$. ~\defpoints{5}
		      \item[(b)] Using~\eqref{eq:1}, argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar{x},\bar{y})$,
		            where $\bar{x} = \tfrac{1}{n}\sum_{i=1}^{n}x_{i}$ and $\bar{y} = \tfrac{1}{n}\sum_{i=1}^{n} y_{i}$. ~\defpoints{5}
		      \item[(c)] Suppose the observed label value $y_i$ ($i=1,2,...,n$) is generated according to the non-deterministic linear model:
		            \begin{equation}
			            y_i = \theta x_i + \theta_0 + \epsilon, \quad \epsilon \sim \mathcal{N}(0,\sigma^2),
		            \end{equation}
		            where $\mathcal{N}(0,\sigma^2)$ denotes a Gaussian distribution with mean 0 and variance $\sigma^2$.
		            Calculate the expectation and variance of $y_i$ ($i=1,2,...,n$), and
		            use Maximum Likelihood Estimation (MLE) to estimate the model parameters $\theta$ and $\theta_0$. ~\defpoints{5}
		      \item[(d)] Suppose the observed label value $y_i$ ($i=1,2,...,n$) is generated according to the non-deterministic linear model:
		            \begin{equation}
			            y_i = \theta x_i + \theta_0 + \epsilon_i, \quad \epsilon \sim \mathcal{N}(0,\sigma_i^2).
		            \end{equation}
		            Use MLE to estimate the model parameters $\theta$ and $\theta_0$, and discuss the difference with the results in (c). ~\defpoints{5}
	      \end{itemize}


	\item Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized Residual Sum of Squares (RSS),
	      \begin{equation}
		      \hat{\theta}^{ridge},~\hat{\theta}^{ridge}_0 = \argmin_{\theta,~\theta_0} \left( \sum_{i=1}^n \left( y_i-\theta_0 - \sum_{j=1}^p x_{ij} \theta_j \right)^2 + \lambda \sum_{j=1}^p \theta_j^{2} \right).
		      \label{eq:ridge}
	      \end{equation}
	      Here $\lambda \geq 0$ is a complexity parameter that controls the amount of shrinkage.
	      \begin{itemize}
		      \item[(a)] Estimate the closed-form solution to $\theta_j$ ($j=1,2,...,p$) and $\theta_0$ in (\ref{eq:ridge}). ~\defpoints{5}
		      \item[(b)] Show that the ridge regression problem in (\ref{eq:ridge}) is equivalent to the problem:
		            \begin{equation}
			            \hat{\theta}^c,~\hat{\theta}_0 = \argmin_{\theta^c,~\theta_0} \left( \sum_{i=1}^n \left( y_i-\theta_0^c - \sum_{j=1}^p (x_{ij}-\bar{x}_j)\theta_j^c \right)^2 + \lambda \sum_{j=1}^p \theta_j^{c2} \right),
		            \end{equation}
		            where $\bar{x}_j = \sum_{i=1}^n x_{ij}$, $j=1,2,...,p$.
		            Given the correspondence between $\theta^c$ and the original $\theta$ in (\ref{eq:ridge}). Characterize the solution to this modified criterion. ~\defpoints{5}
		      \item[(c)] After reparameterization using centered inputs ($x_{ij} \leftarrow x_{ij}-\bar{x}_j$, $\forall i,j$),
		            show that the solution to (\ref{eq:ridge}) can be separated into following two parts:
		            \begin{align}
			            \hat{\theta}^{ridge}_0 & = \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i,                                                                                                                \\
			            \hat{\theta}^{ridge}   & = \argmin_{\theta} \left( \sum_{i=1}^n \left( y_i- \sum_{j=1}^p x_{ij} \theta_j \right)^2 + \lambda \sum_{j=1}^p \theta_j^{2} \right). \label{eq:ridge2}
		            \end{align} ~\defpoints{5}
		      \item[(d)] Given $\mathbf{X} = [\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_n]^\top \in \mathbb{R}^{n \times p}$ ($\mathbf{x}_i \in \mathbb{R}^p$ is the $i$-th example, $i=1,2,...,n$),
		            $\mathbf{y} = [y_1,y_2,...,y_n]^\top \in \mathbb{R}^{n}$, and $\bm\theta = [\theta_1,\theta_2,...,\theta_p]^\top \in \mathbb{R}^p$.
		            Show the optimization problem (\ref{eq:ridge2}) and its closed-form solution in the matrix form. ~\defpoints{5}
	      \end{itemize}


		  \iffalse
	\item Consider the ridge regression problem, under a Gaussian prior $\bm\theta \sim \mathcal{N}(0,\tau \mathbf{I})$,
	      and Gaussian sampling model $\mathbf{y} \sim \mathcal{N}(\mathbf{X}\bm\theta,\sigma^2\mathbf{I})$.
	      Find the relation between the regularization parameter $\lambda$ in the ridge formula, and the variances $\tau$ and $\sigma^2$. ~\defpoints{8}
\fi

\end{enumerate}

\end{document}

